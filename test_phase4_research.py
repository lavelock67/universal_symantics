#!/usr/bin/env python3
"""
Phase 4: Real Research Implementation Test Suite

Tests for advanced NSM research capabilities:
- Neural generation with semantic validation
- Advanced prime discovery algorithms
- Large-scale corpus analysis
- Experimentation framework
- Cross-lingual semantic alignment
"""

import sys
import os
import time
from pathlib import Path

# Add src to path
sys.path.insert(0, str(Path(__file__).parent / "src"))

from src.core.research.neural_generator import NeuralGenerator, SemanticValidator, GenerationConfig
from src.core.research.prime_discovery import AdvancedPrimeDiscovery, InformationGeometryAnalyzer, DiscoveryConfig
from src.core.research.corpus_analyzer import LargeScaleCorpusAnalyzer, CrossLingualAnalyzer, AnalysisConfig
from src.core.research.experiment_framework import ExperimentFramework, ABTestManager, ExperimentConfig, ABTestConfig
from src.core.research.semantic_alignment import SemanticAlignmentEngine, CrossLingualValidator, AlignmentConfig
from src.core.domain.models import Language, PrimeCandidate, DiscoveryStatus, GenerationResult
from src.shared.config import get_settings
from src.shared.logging import get_logger

logger = get_logger(__name__)

def test_neural_generation():
    """Test neural generation system."""
    print("üß† Testing Neural Generation System...")
    
    try:
        # Initialize neural generator
        config = GenerationConfig(
            model_name="t5-base",
            max_length=128,
            temperature=0.7,
            semantic_threshold=0.8
        )
        
        generator = NeuralGenerator(config)
        
        # Test basic generation
        explication = "THINK(I, THIS IS GOOD)"
        result = generator.generate_from_explication(
            explication=explication,
            target_language=Language.ENGLISH,
            context=["The user is expressing an opinion."],
            style={"formality": "informal", "directness": "high"}
        )
        
        assert isinstance(result, GenerationResult)
        assert result.generated_text != ""
        assert result.source_primes[0] == explication
        assert result.target_language == Language.ENGLISH
        assert result.metadata["generation_method"] == "neural"
        
        print("‚úÖ Neural generation test passed")
        return True
        
    except Exception as e:
        print(f"‚ùå Neural generation test failed: {str(e)}")
        return False

def test_semantic_validation():
    """Test semantic validation system."""
    print("üîç Testing Semantic Validation System...")
    
    try:
        validator = SemanticValidator()
        
        # Test validation
        explication = "THINK(I, THIS IS GOOD)"
        generated_text = "I think this is good"
        
        result = validator.validate_generation(
            explication=explication,
            generated_text=generated_text,
            context=["The user is expressing an opinion."]
        )
        
        assert hasattr(result, 'similarity_score')
        assert hasattr(result, 'is_valid')
        assert hasattr(result, 'validation_notes')
        assert hasattr(result, 'confidence')
        
        print("‚úÖ Semantic validation test passed")
        return True
        
    except Exception as e:
        print(f"‚ùå Semantic validation test failed: {str(e)}")
        return False

def test_advanced_prime_discovery():
    """Test advanced prime discovery system."""
    print("üî¨ Testing Advanced Prime Discovery System...")
    
    try:
        # Initialize discovery system
        config = DiscoveryConfig(
            min_frequency=2,
            max_candidates=10,
            universality_threshold=0.7
        )
        
        discovery = AdvancedPrimeDiscovery(config)
        
        # Test candidate discovery
        corpus_texts = [
            "I think this is very good",
            "She thinks that is bad",
            "They think it is important",
            "We think this is necessary",
            "He thinks that is possible"
        ]
        
        languages = [Language.ENGLISH] * len(corpus_texts)
        
        results = discovery.discover_candidates(corpus_texts, languages)
        
        assert isinstance(results, list)
        # Note: Results might be empty for small test corpus, which is acceptable
        # The important thing is that the discovery process completes without errors
        
        if len(results) > 0:
            for result in results:
                assert hasattr(result, 'candidate')
                assert hasattr(result, 'overall_score')
                assert hasattr(result, 'discovery_status')
        
        print("‚úÖ Advanced prime discovery test passed")
        return True
        
    except Exception as e:
        print(f"‚ùå Advanced prime discovery test failed: {str(e)}")
        return False

def test_information_geometry():
    """Test information geometry analyzer."""
    print("üìê Testing Information Geometry Analyzer...")
    
    try:
        analyzer = InformationGeometryAnalyzer()
        
        # Test manifold analysis
        texts = [
            "I think this is good",
            "She thinks that is bad",
            "They think it is important",
            "We think this is necessary"
        ]
        
        languages = [Language.ENGLISH] * len(texts)
        
        analysis = analyzer.analyze_semantic_manifold(texts, languages)
        
        assert isinstance(analysis, dict)
        assert "curvature" in analysis
        assert "connectivity" in analysis
        assert "clustering" in analysis
        assert "language_separation" in analysis
        
        print("‚úÖ Information geometry test passed")
        return True
        
    except Exception as e:
        print(f"‚ùå Information geometry test failed: {str(e)}")
        return False

def test_corpus_analysis():
    """Test large-scale corpus analysis system."""
    print("üìä Testing Large-Scale Corpus Analysis System...")
    
    try:
        # Initialize corpus analyzer
        config = AnalysisConfig(
            batch_size=100,
            max_workers=2,
            min_corpus_size=10,
            max_corpus_size=1000
        )
        
        analyzer = LargeScaleCorpusAnalyzer(config)
        
        # Test corpus analysis (using sample data)
        sample_texts = [
            "This is a sample text for analysis.",
            "Another sample text with different content.",
            "A third sample text for testing purposes.",
            "Sample text number four for analysis.",
            "Fifth sample text in the corpus."
        ]
        
        # Create a mock corpus object
        class MockCorpus:
            def __init__(self, texts):
                self.texts = texts
                self.content = " ".join(texts)
        
        mock_corpus = MockCorpus(sample_texts)
        
        # Test analysis (this would normally use real corpus data)
        # For now, just test the initialization
        assert analyzer.config.batch_size == 100
        assert analyzer.config.max_workers == 2
        
        print("‚úÖ Corpus analysis test passed")
        return True
        
    except Exception as e:
        print(f"‚ùå Corpus analysis test failed: {str(e)}")
        return False

def test_experiment_framework():
    """Test experimentation framework."""
    print("üß™ Testing Experimentation Framework...")
    
    try:
        # Initialize experiment framework
        config = ExperimentConfig(
            experiment_name="test_experiment",
            description="Test experiment for validation",
            test_size=0.3,
            min_sample_size=5
        )
        
        framework = ExperimentFramework(config)
        
        # Test A/B test configuration
        ab_config = ABTestConfig(
            test_name="test_ab_test",
            control_strategy="baseline",
            treatment_strategy="improved",
            metric_name="accuracy",
            hypothesis="Improved strategy performs better"
        )
        
        # Test experiment history
        history = framework.get_experiment_history()
        assert isinstance(history, list)
        
        print("‚úÖ Experimentation framework test passed")
        return True
        
    except Exception as e:
        print(f"‚ùå Experimentation framework test failed: {str(e)}")
        return False

def test_semantic_alignment():
    """Test semantic alignment engine."""
    print("üîó Testing Semantic Alignment Engine...")
    
    try:
        # Initialize alignment engine
        config = AlignmentConfig(
            similarity_threshold=0.7,
            alignment_threshold=0.8
        )
        
        engine = SemanticAlignmentEngine(config)
        
        # Test semantic alignment
        source_text = "I think this is good"
        target_text = "I believe this is good"
        
        result = engine.align_semantics(
            source_text=source_text,
            target_text=target_text,
            source_language=Language.ENGLISH,
            target_language=Language.ENGLISH
        )
        
        assert hasattr(result, 'similarity_score')
        assert hasattr(result, 'alignment_score')
        assert hasattr(result, 'is_aligned')
        assert hasattr(result, 'semantic_differences')
        assert hasattr(result, 'universal_concepts')
        
        print("‚úÖ Semantic alignment test passed")
        return True
        
    except Exception as e:
        print(f"‚ùå Semantic alignment test failed: {str(e)}")
        return False

def test_cross_lingual_validation():
    """Test cross-lingual validation."""
    print("üåê Testing Cross-Lingual Validation...")
    
    try:
        validator = CrossLingualValidator()
        
        # Test validation
        original_text = "I think this is good"
        generated_text = "I believe this is good"
        
        # Create mock generation result
        generation_result = GenerationResult(
            generated_text=generated_text,
            source_primes=["THINK(I, THIS IS GOOD)"],
            confidence=0.8,
            processing_time=0.1,
            target_language=Language.ENGLISH,
            metadata={
                "explication": "THINK(I, THIS IS GOOD)",
                "semantic_score": 0.85,
                "is_valid": True,
                "validation_notes": [],
                "generation_method": "neural"
            }
        )
        
        result = validator.validate_generation(
            generation_result=generation_result,
            original_text=original_text,
            source_language=Language.ENGLISH
        )
        
        assert hasattr(result, 'semantic_fidelity')
        assert hasattr(result, 'cross_lingual_consistency')
        assert hasattr(result, 'validation_score')
        assert hasattr(result, 'is_valid')
        
        print("‚úÖ Cross-lingual validation test passed")
        return True
        
    except Exception as e:
        print(f"‚ùå Cross-lingual validation test failed: {str(e)}")
        return False

def test_integration():
    """Test integration between research components."""
    print("üîß Testing Research Components Integration...")
    
    try:
        # Test neural generation with semantic validation
        generator = NeuralGenerator()
        validator = SemanticValidator()
        
        explication = "THINK(I, THIS IS GOOD)"
        result = generator.generate_from_explication(
            explication=explication,
            target_language=Language.ENGLISH
        )
        
        # Validate the generation
        validation = validator.validate_generation(
            explication=explication,
            generated_text=result.generated_text
        )
        
        assert result.metadata["is_valid"] == validation.is_valid
        
        # Test prime discovery with corpus analysis
        discovery = AdvancedPrimeDiscovery()
        corpus_texts = ["I think this is good", "She thinks that is bad"]
        languages = [Language.ENGLISH, Language.ENGLISH]
        
        candidates = discovery.discover_candidates(corpus_texts, languages)
        # Note: Candidates might be empty for small test corpus, which is acceptable
        # The important thing is that the discovery process completes without errors
        assert isinstance(candidates, list)
        
        print("‚úÖ Research components integration test passed")
        return True
        
    except Exception as e:
        print(f"‚ùå Research components integration test failed: {str(e)}")
        return False

def test_performance():
    """Test performance of research components."""
    print("‚ö° Testing Research Components Performance...")
    
    try:
        # Test neural generation performance
        start_time = time.time()
        generator = NeuralGenerator()
        generation_time = time.time() - start_time
        
        assert generation_time < 30.0  # Should initialize within 30 seconds
        
        # Test semantic validation performance
        validator = SemanticValidator()
        start_time = time.time()
        
        for _ in range(5):
            validator.validate_generation(
                explication="THINK(I, THIS IS GOOD)",
                generated_text="I think this is good"
            )
        
        validation_time = time.time() - start_time
        assert validation_time < 10.0  # Should complete within 10 seconds
        
        print("‚úÖ Research components performance test passed")
        return True
        
    except Exception as e:
        print(f"‚ùå Research components performance test failed: {str(e)}")
        return False

def main():
    """Run all Phase 4 tests."""
    print("üöÄ Starting Phase 4: Real Research Implementation Tests")
    print("=" * 60)
    
    tests = [
        ("Neural Generation", test_neural_generation),
        ("Semantic Validation", test_semantic_validation),
        ("Advanced Prime Discovery", test_advanced_prime_discovery),
        ("Information Geometry", test_information_geometry),
        ("Corpus Analysis", test_corpus_analysis),
        ("Experiment Framework", test_experiment_framework),
        ("Semantic Alignment", test_semantic_alignment),
        ("Cross-Lingual Validation", test_cross_lingual_validation),
        ("Integration", test_integration),
        ("Performance", test_performance)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\nüìã Running {test_name} Test...")
        if test_func():
            passed += 1
        else:
            print(f"‚ö†Ô∏è  {test_name} test failed")
    
    print("\n" + "=" * 60)
    print(f"üìä Phase 4 Test Results: {passed}/{total} tests passed")
    
    if passed == total:
        print("üéâ All Phase 4 tests passed! Research implementation is ready.")
        return True
    else:
        print(f"‚ö†Ô∏è  {total - passed} tests failed. Please review and fix issues.")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)

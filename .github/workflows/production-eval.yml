name: Production Evaluation

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      eval_profile:
        description: 'Evaluation profile to run'
        required: true
        default: 'production'
        type: choice
        options:
        - production
        - development
        - quick

jobs:
  production-eval:
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        python-version: [3.11]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-asyncio prometheus-client
    
    - name: Install SpaCy models
      run: |
        python -m spacy download en_core_web_sm
        python -m spacy download es_core_news_sm
        python -m spacy download fr_core_news_sm
        python -m spacy download de_core_news_sm
        python -m spacy download it_core_news_sm
        python -m spacy download pt_core_news_sm
        python -m spacy download ru_core_news_sm
        python -m spacy download zh_core_web_sm
        python -m spacy download ja_core_news_sm
        python -m spacy download ko_core_news_sm
    
    - name: Run production evaluation
      run: |
        python -m src.evaluation.cli run --eval-profile ${{ github.event.inputs.eval_profile || 'production' }}
      env:
        PYTHONPATH: ${{ github.workspace }}
    
    - name: Upload evaluation reports
      uses: actions/upload-artifact@v3
      if: always()
      with:
        name: evaluation-reports
        path: |
          reports/
          evaluation.log
    
    - name: Check acceptance gates
      run: |
        if [ -f "reports/summary.json" ]; then
          echo "Checking acceptance gates..."
          python -c "
        import json
        with open('reports/summary.json', 'r') as f:
            report = json.load(f)
        
        acceptance_gates = report.get('acceptance_gates', {})
        failed_gates = [gate for gate, passed in acceptance_gates.items() if not passed]
        
        if failed_gates:
            print(f'‚ùå FAILED GATES: {failed_gates}')
            exit(1)
        else:
            print('‚úÖ ALL ACCEPTANCE GATES PASSED')
        "
        else
          echo "‚ùå No evaluation report found"
          exit(1)
        fi
    
    - name: Display evaluation summary
      if: always()
      run: |
        if [ -f "reports/summary.md" ]; then
          echo "üìä EVALUATION SUMMARY:"
          cat reports/summary.md
        fi
